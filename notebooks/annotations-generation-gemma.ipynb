{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Annotations generation\n","This notebook uses the Gemma LLM to generate new annotations over previously unused narrations.  \n","In particular, for each group of 3 consecutive narrations sampled, two annotations are generated."]},{"cell_type":"markdown","metadata":{},"source":["## Setup and load data"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-12T07:32:24.364378Z","iopub.status.busy":"2024-06-12T07:32:24.364033Z","iopub.status.idle":"2024-06-12T07:32:25.268896Z","shell.execute_reply":"2024-06-12T07:32:25.267977Z","shell.execute_reply.started":"2024-06-12T07:32:24.364348Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/gemma/pytorch/2b-it/2/config.json\n","/kaggle/input/gemma/pytorch/2b-it/2/gemma-2b-it.ckpt\n","/kaggle/input/gemma/pytorch/2b-it/2/tokenizer.model\n","/kaggle/input/annotations-ego4d/annotations/ego4d.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/av_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/nlq_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_hands_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/vq_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/nlq_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/av_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_sta_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/manifest.ver\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_oscc-pnr_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_hands_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/av_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_scod_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_hands_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_scod_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_oscc-pnr_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/narration.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/moments_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/moments_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/narration_verb_taxonomy.csv\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/vq_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/narration_noun_taxonomy.csv\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_lta_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_sta_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_oscc-pnr_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/vq_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_lta_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_lta_taxonomy.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/moments_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_scod_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_lta_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/nlq_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/manifest.csv\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_sta_val.json\n","/kaggle/input/not-present-videos/ego4d_uids_video_difference_v1_v2.txt\n"]}],"source":["# list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T07:32:25.271426Z","iopub.status.busy":"2024-06-12T07:32:25.270573Z","iopub.status.idle":"2024-06-12T07:33:33.939987Z","shell.execute_reply":"2024-06-12T07:33:33.939124Z","shell.execute_reply.started":"2024-06-12T07:32:25.271399Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'gemma_pytorch'...\n","remote: Enumerating objects: 177, done.\u001b[K\n","remote: Counting objects: 100% (109/109), done.\u001b[K\n","remote: Compressing objects: 100% (61/61), done.\u001b[K\n","remote: Total 177 (delta 64), reused 68 (delta 46), pack-reused 68\u001b[K\n","Receiving objects: 100% (177/177), 2.16 MiB | 2.39 MiB/s, done.\n","Resolving deltas: 100% (91/91), done.\n","Machine type: cuda\n","Weights directory exists. Listing contents:\n","['config.json', 'gemma-2b-it.ckpt', 'tokenizer.model']\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]}],"source":["# setup the environment and install necessary packages\n","!pip install -q -U immutabledict sentencepiece \n","!git clone https://github.com/google/gemma_pytorch.git\n","!mkdir /kaggle/working/gemma/\n","!mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/\n","\n","import sys\n","import contextlib\n","import os\n","import torch\n","\n","# add the gemma_pytorch directory to the system path\n","sys.path.append(\"/kaggle/working/gemma_pytorch/\") \n","\n","# import necessary modules from Gemma\n","from gemma.config import GemmaConfig, get_config_for_7b, get_config_for_2b\n","from gemma.model import GemmaForCausalLM\n","from gemma.tokenizer import Tokenizer\n","\n","# load the model\n","\n","# define constants\n","VARIANT = \"2b-it\" \n","MACHINE_TYPE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Machine type: {MACHINE_TYPE}\")\n","weights_dir = '/kaggle/input/gemma/pytorch/2b-it/2/' \n","# kagglehub.download('gemma/pytorch/1.1-2b-it/1/')\n","\n","# verify the weights directory and its contents\n","if os.path.exists(weights_dir):\n","    print(\"Weights directory exists. Listing contents:\")\n","    print(os.listdir(weights_dir))\n","else:\n","    print(\"Weights directory does not exist.\")\n","\n","# context manager to set the default tensor type\n","@contextlib.contextmanager\n","def _set_default_tensor_type(dtype: torch.dtype):\n","  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n","  torch.set_default_dtype(dtype)\n","  yield\n","  torch.set_default_dtype(torch.float)\n","\n","# load the model configuration\n","model_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\n","model_config.tokenizer = os.path.join(weights_dir, \"tokenizer.model\")\n","\n","# set the device\n","device = torch.device(MACHINE_TYPE)\n","\n","# load the model weights and set the model to evaluation mode\n","with _set_default_tensor_type(model_config.get_dtype()):\n","  model = GemmaForCausalLM(model_config)\n","  ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')\n","  model.load_weights(ckpt_path)\n","  model = model.to(device).eval()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T07:33:33.941624Z","iopub.status.busy":"2024-06-12T07:33:33.941179Z","iopub.status.idle":"2024-06-12T07:33:33.958386Z","shell.execute_reply":"2024-06-12T07:33:33.957569Z","shell.execute_reply.started":"2024-06-12T07:33:33.941595Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/gemma/pytorch/2b-it/2/config.json\n","/kaggle/input/gemma/pytorch/2b-it/2/gemma-2b-it.ckpt\n","/kaggle/input/gemma/pytorch/2b-it/2/tokenizer.model\n","/kaggle/input/annotations-ego4d/annotations/ego4d.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/av_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/nlq_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_hands_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/vq_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/nlq_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/av_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_sta_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/manifest.ver\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_oscc-pnr_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_hands_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/av_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_scod_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_hands_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_scod_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_oscc-pnr_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/narration.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/moments_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/moments_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/narration_verb_taxonomy.csv\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/vq_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/narration_noun_taxonomy.csv\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_lta_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_sta_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_oscc-pnr_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/vq_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_lta_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_lta_taxonomy.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/moments_val.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_scod_train.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_lta_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/nlq_test_unannotated.json\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/manifest.csv\n","/kaggle/input/annotations-ego4d/annotations/v1/annotations/fho_sta_val.json\n","/kaggle/input/not-present-videos/ego4d_uids_video_difference_v1_v2.txt\n"]}],"source":["# check input list\n","input_directory = '/kaggle/input/'\n","for root, dirs, files in os.walk(input_directory):\n","    for name in files:\n","        print(os.path.join(root, name))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# useful imports\n","\n","import pandas as pd\n","import numpy as np\n","import json\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# get video uids of the val file\n","\n","nlq_val_path = '/kaggle/input/annotations-ego4d/annotations/v1/annotations/nlq_val.json'\n","with open(nlq_val_path, 'r') as f:\n","    data = json.load(f)\n","\n","videos_uids_val = []\n","for video in data['videos']:    \n","    video_uid = video.get('video_uid')\n","    videos_uids_val.append(video_uid)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T07:33:34.092623Z","iopub.status.busy":"2024-06-12T07:33:34.092320Z","iopub.status.idle":"2024-06-12T07:33:34.115141Z","shell.execute_reply":"2024-06-12T07:33:34.114424Z","shell.execute_reply.started":"2024-06-12T07:33:34.092597Z"},"trusted":true},"outputs":[],"source":["# get video uids of the test file\n","\n","nlq_test_path = '/kaggle/input/annotations-ego4d/annotations/v1/annotations/nlq_test_unannotated.json'\n","with open(nlq_test_path, 'r') as f:\n","    data = json.load(f)\n","\n","videos_uids_test = []\n","for video in data['videos']:    \n","    video_uid = video.get('video_uid')\n","    videos_uids_test.append(video_uid)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T07:33:34.116705Z","iopub.status.busy":"2024-06-12T07:33:34.116417Z","iopub.status.idle":"2024-06-12T07:33:34.125832Z","shell.execute_reply":"2024-06-12T07:33:34.124779Z","shell.execute_reply.started":"2024-06-12T07:33:34.116681Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['4a08e95b-cb52-4621-b679-87f617893e19', 'cbacc898-22f4-4a83-81ae-8d421791feb2', '20164675-63be-45a8-93d1-6ab5cad76012', '0b3fb06d-aed7-44ce-ba44-c079aea84da2', '864a2391-63d5-4f64-9ba2-cf1367c178c2', '24f035b3-fa40-4846-9546-fb61271e5caa', '2c3173b5-08d1-4051-a700-c677574df125', '29bc686e-8f5c-49de-a23e-c0b802e47d4d', 'fb6ff052-5ea7-4647-a0a3-ebc74133c86b', 'e7ef4970-4d3e-4fd3-9a90-bb81a0873c86', '34b0812d-21a9-48ad-a2f3-435455f169f0', '6bbea7d8-d5fb-4684-9a53-278fbffad5a1', 'a66da989-08eb-4f6a-98ea-d6402b81ad89', 'ff3a727c-2bcd-4a33-95eb-5d18d385ea70', '136f0c55-b87c-45cc-bcaa-ff2c39bae889', '703d550a-0a84-4bcf-9b45-e25c864ade70', 'e2454ff3-44f1-45d0-9e77-011657400b01', '7a39a702-dcd1-46c4-82aa-00c97bda423e', '54c5d370-6738-48b2-8eab-39d75138e118', '7f267263-b89f-43a7-b591-faa33ef5affe', '32a80da2-6b71-472a-92f3-9e2e5596146c', 'ea27cc27-037f-4c63-b418-faea630faf8e', '865733f5-97b6-4380-a418-1fd6510e0f5e', '6d2c4747-1232-48d3-8922-afcafc112bce', '0a74808e-4f55-4bd4-abbd-78f3435ea5bc', '2665edd9-4ab5-493b-aa3d-dd156be107a7', '6170935a-743e-4368-8d3f-8586b83cf492', 'd0fe52fc-cb32-4bcc-ac7c-0312968a8b98', '5b429988-dc08-4def-a5b7-de0b585f35e7', 'a03169a7-5550-4aff-b2ea-525758666ece', '10db9909-0e97-46a9-8862-147b0b03da48', '06d5d4b8-d5e1-4ebd-816b-9165cd0ff216', 'df628caf-cbab-419a-ba23-a3a82a31a53e', '6223f809-8580-4b8b-93d2-a7e4bb21c240', '5b3863e7-350e-492e-81c6-16ea0d6a55b0', '42ff99e2-2a5a-4aa3-8a96-c5d7669ae3c8', '71b97752-11d4-42b1-8713-c3168b89ac4d', '94ed2b0d-d41e-4792-874c-3f189975aa7c', '1d1c306f-a11b-4fb8-885e-affa145d7960', '67cf6d70-7387-45ab-8200-27ce803476f8', '3d67bdde-232a-442e-a29d-56f8d1a323bf', '4d29d4f8-7bcc-45df-9182-0937b462d7a1', 'fc3be64b-873c-4947-ab05-bda776b09924', '616d8c2c-5c88-44aa-987d-f571dc73a6f9', 'b4abd484-c055-48da-abc5-aff7bd6cbc72', 'a3beb693-7e2a-4d6f-8658-88d92b453d57', 'b891de05-9e70-4d30-bb28-133983989108', 'adccfce5-4fbe-481a-81f1-dda873f19d4c', '8954290a-c911-48a8-a85f-dc4274ac682d', '8479f6ff-ffa8-47b4-a555-98d45faafb47', '784aec02-02f4-4821-8eb1-1568faaf0892', '2e96ebef-240a-4c8f-8d75-405d2d671021']\n"]}],"source":["# get video uids present in version 1 but not 2 (egovlp pretrained features only for videos of second version)\n","not_present_videos_path = '/kaggle/input/not-present-videos/ego4d_uids_video_difference_v1_v2.txt'\n","with open(not_present_videos_path, 'r') as file:    \n","    lines = file.readlines()\n","\n","not_present_videos = [line.strip() for line in lines]\n","\n","print(not_present_videos)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T07:33:34.127325Z","iopub.status.busy":"2024-06-12T07:33:34.127064Z","iopub.status.idle":"2024-06-12T07:34:03.812389Z","shell.execute_reply":"2024-06-12T07:34:03.811568Z","shell.execute_reply.started":"2024-06-12T07:33:34.127303Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 9645/9645 [00:04<00:00, 2372.20it/s]\n"]}],"source":["# create a dataframe of narrations and their info\n","narrations_path = '/kaggle/input/annotations-ego4d/annotations/v1/annotations/narration.json'\n","with open(narrations_path, 'r') as f:\n","    data = json.load(f)\n","\n","# list of narrations with their info\n","records = []\n","for video_uid, content in tqdm(data.items()):\n","        #Filter out videos which are \"not complete\", videos belonging to nlq_val.json, nlq_test.json and videos for which we don't have visual features\n","        if (content.get('status') != 'complete') or (video_uid in videos_uids_val) or (video_uid in videos_uids_test) or (video_uid in not_present_videos):\n","            continue\n","        \n","        # retrieve narrations of the first annotator\n","        narration_pass_1 = content.get('narration_pass_1', {})\n","        narrations = narration_pass_1.get('narrations', [])\n","        \n","        for narration in narrations:\n","            # get the text of the narration\n","            narration_text = narration.get('narration_text')\n","            # remove the #C or similar patterns at the beginning of each narration\n","            if narration_text.startswith(\"#\"):\n","                narration_text = narration_text[2:]\n","            \n","            # create a record with all the narration info\n","            record = {\n","                'video_uid': video_uid,\n","                'annotation_uid': narration.get('annotation_uid'),\n","                'narration_text': narration_text,\n","                'timestamp_sec': narration.get('timestamp_sec'),\n","                'timestamp_frame': narration.get('timestamp_frame'),\n","            }\n","            records.append(record)\n","\n","df = pd.DataFrame(records)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T07:34:03.813962Z","iopub.status.busy":"2024-06-12T07:34:03.813621Z","iopub.status.idle":"2024-06-12T07:34:03.838173Z","shell.execute_reply":"2024-06-12T07:34:03.837299Z","shell.execute_reply.started":"2024-06-12T07:34:03.813932Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_uid</th>\n","      <th>annotation_uid</th>\n","      <th>narration_text</th>\n","      <th>timestamp_sec</th>\n","      <th>timestamp_frame</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>77cc4654-4eec-44c6-af05-dbdf71f9a401</td>\n","      <td>920182f7-5385-488b-99f9-caf8f0d9fe6b</td>\n","      <td>C interacts with a woman X</td>\n","      <td>0.00000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>77cc4654-4eec-44c6-af05-dbdf71f9a401</td>\n","      <td>920182f7-5385-488b-99f9-caf8f0d9fe6b</td>\n","      <td>C walks into the kitchen</td>\n","      <td>4.53806</td>\n","      <td>136</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>77cc4654-4eec-44c6-af05-dbdf71f9a401</td>\n","      <td>920182f7-5385-488b-99f9-caf8f0d9fe6b</td>\n","      <td>C opens a shelf</td>\n","      <td>12.92098</td>\n","      <td>388</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>77cc4654-4eec-44c6-af05-dbdf71f9a401</td>\n","      <td>920182f7-5385-488b-99f9-caf8f0d9fe6b</td>\n","      <td>C brings out a basket from the shelf</td>\n","      <td>15.10264</td>\n","      <td>453</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>77cc4654-4eec-44c6-af05-dbdf71f9a401</td>\n","      <td>920182f7-5385-488b-99f9-caf8f0d9fe6b</td>\n","      <td>C puts back the basket into the shelf</td>\n","      <td>17.15749</td>\n","      <td>515</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2329802</th>\n","      <td>ad4f61f0-4c0f-4ce1-bdaa-e57b79250527</td>\n","      <td>9cdb0f18-81e5-4288-bf2e-3b44e5977107</td>\n","      <td>C arranges a duvet on the bed</td>\n","      <td>8.31072</td>\n","      <td>249</td>\n","    </tr>\n","    <tr>\n","      <th>2329803</th>\n","      <td>ad4f61f0-4c0f-4ce1-bdaa-e57b79250527</td>\n","      <td>9cdb0f18-81e5-4288-bf2e-3b44e5977107</td>\n","      <td>C picks a blanket from the bed</td>\n","      <td>39.01209</td>\n","      <td>1170</td>\n","    </tr>\n","    <tr>\n","      <th>2329804</th>\n","      <td>ad4f61f0-4c0f-4ce1-bdaa-e57b79250527</td>\n","      <td>9cdb0f18-81e5-4288-bf2e-3b44e5977107</td>\n","      <td>C arranges the duvet on the bed</td>\n","      <td>35.01310</td>\n","      <td>1050</td>\n","    </tr>\n","    <tr>\n","      <th>2329805</th>\n","      <td>ad4f61f0-4c0f-4ce1-bdaa-e57b79250527</td>\n","      <td>9cdb0f18-81e5-4288-bf2e-3b44e5977107</td>\n","      <td>C folds the blanket on the bed</td>\n","      <td>42.24332</td>\n","      <td>1267</td>\n","    </tr>\n","    <tr>\n","      <th>2329806</th>\n","      <td>ad4f61f0-4c0f-4ce1-bdaa-e57b79250527</td>\n","      <td>9cdb0f18-81e5-4288-bf2e-3b44e5977107</td>\n","      <td>C lays the blanket by the edge of the bed</td>\n","      <td>57.80142</td>\n","      <td>1734</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2329807 rows × 5 columns</p>\n","</div>"],"text/plain":["                                    video_uid  \\\n","0        77cc4654-4eec-44c6-af05-dbdf71f9a401   \n","1        77cc4654-4eec-44c6-af05-dbdf71f9a401   \n","2        77cc4654-4eec-44c6-af05-dbdf71f9a401   \n","3        77cc4654-4eec-44c6-af05-dbdf71f9a401   \n","4        77cc4654-4eec-44c6-af05-dbdf71f9a401   \n","...                                       ...   \n","2329802  ad4f61f0-4c0f-4ce1-bdaa-e57b79250527   \n","2329803  ad4f61f0-4c0f-4ce1-bdaa-e57b79250527   \n","2329804  ad4f61f0-4c0f-4ce1-bdaa-e57b79250527   \n","2329805  ad4f61f0-4c0f-4ce1-bdaa-e57b79250527   \n","2329806  ad4f61f0-4c0f-4ce1-bdaa-e57b79250527   \n","\n","                               annotation_uid  \\\n","0        920182f7-5385-488b-99f9-caf8f0d9fe6b   \n","1        920182f7-5385-488b-99f9-caf8f0d9fe6b   \n","2        920182f7-5385-488b-99f9-caf8f0d9fe6b   \n","3        920182f7-5385-488b-99f9-caf8f0d9fe6b   \n","4        920182f7-5385-488b-99f9-caf8f0d9fe6b   \n","...                                       ...   \n","2329802  9cdb0f18-81e5-4288-bf2e-3b44e5977107   \n","2329803  9cdb0f18-81e5-4288-bf2e-3b44e5977107   \n","2329804  9cdb0f18-81e5-4288-bf2e-3b44e5977107   \n","2329805  9cdb0f18-81e5-4288-bf2e-3b44e5977107   \n","2329806  9cdb0f18-81e5-4288-bf2e-3b44e5977107   \n","\n","                                     narration_text  timestamp_sec  \\\n","0                        C interacts with a woman X        0.00000   \n","1                          C walks into the kitchen        4.53806   \n","2                                  C opens a shelf        12.92098   \n","3              C brings out a basket from the shelf       15.10264   \n","4             C puts back the basket into the shelf       17.15749   \n","...                                             ...            ...   \n","2329802               C arranges a duvet on the bed        8.31072   \n","2329803              C picks a blanket from the bed       39.01209   \n","2329804             C arranges the duvet on the bed       35.01310   \n","2329805              C folds the blanket on the bed       42.24332   \n","2329806   C lays the blanket by the edge of the bed       57.80142   \n","\n","         timestamp_frame  \n","0                      0  \n","1                    136  \n","2                    388  \n","3                    453  \n","4                    515  \n","...                  ...  \n","2329802              249  \n","2329803             1170  \n","2329804             1050  \n","2329805             1267  \n","2329806             1734  \n","\n","[2329807 rows x 5 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["pd.set_option('display.max_columns', None)\n","df"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T07:34:03.839650Z","iopub.status.busy":"2024-06-12T07:34:03.839360Z","iopub.status.idle":"2024-06-12T07:34:21.331558Z","shell.execute_reply":"2024-06-12T07:34:21.330404Z","shell.execute_reply.started":"2024-06-12T07:34:03.839626Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_34/186839445.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  df_sorted = df.groupby('annotation_uid', group_keys=False).apply(lambda x: x.sort_values('timestamp_sec'))\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_uid</th>\n","      <th>annotation_uid</th>\n","      <th>narration_text</th>\n","      <th>timestamp_sec</th>\n","      <th>timestamp_frame</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>268b8232-2e94-47c6-928b-99ec175340c5</td>\n","      <td>0003222a-d2cb-4e5d-9846-cd0c5ca9ae9c</td>\n","      <td>C flips the page</td>\n","      <td>2715.831482</td>\n","      <td>81474</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>268b8232-2e94-47c6-928b-99ec175340c5</td>\n","      <td>0003222a-d2cb-4e5d-9846-cd0c5ca9ae9c</td>\n","      <td>C wipes the book with the cloth</td>\n","      <td>2717.087122</td>\n","      <td>81512</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>268b8232-2e94-47c6-928b-99ec175340c5</td>\n","      <td>0003222a-d2cb-4e5d-9846-cd0c5ca9ae9c</td>\n","      <td>C puts the book in the shelf</td>\n","      <td>2725.421392</td>\n","      <td>81762</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>268b8232-2e94-47c6-928b-99ec175340c5</td>\n","      <td>0003222a-d2cb-4e5d-9846-cd0c5ca9ae9c</td>\n","      <td>C picks the book</td>\n","      <td>2729.706112</td>\n","      <td>81891</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>268b8232-2e94-47c6-928b-99ec175340c5</td>\n","      <td>0003222a-d2cb-4e5d-9846-cd0c5ca9ae9c</td>\n","      <td>C flips the page</td>\n","      <td>2732.238932</td>\n","      <td>81967</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2329802</th>\n","      <td>a06ac945-e65f-4259-9563-00bbcf62436f</td>\n","      <td>ffff7cd6-7d12-42d4-ae92-f1bfc2fd6e88</td>\n","      <td>The man Q walks around the dining table.</td>\n","      <td>533.928115</td>\n","      <td>16017</td>\n","    </tr>\n","    <tr>\n","      <th>2329803</th>\n","      <td>a06ac945-e65f-4259-9563-00bbcf62436f</td>\n","      <td>ffff7cd6-7d12-42d4-ae92-f1bfc2fd6e88</td>\n","      <td>The man Q fetches water with a water bottle f...</td>\n","      <td>541.369925</td>\n","      <td>16240</td>\n","    </tr>\n","    <tr>\n","      <th>2329804</th>\n","      <td>a06ac945-e65f-4259-9563-00bbcf62436f</td>\n","      <td>ffff7cd6-7d12-42d4-ae92-f1bfc2fd6e88</td>\n","      <td>C opens a refrigerator with his right hand.</td>\n","      <td>542.337345</td>\n","      <td>16269</td>\n","    </tr>\n","    <tr>\n","      <th>2329805</th>\n","      <td>a06ac945-e65f-4259-9563-00bbcf62436f</td>\n","      <td>ffff7cd6-7d12-42d4-ae92-f1bfc2fd6e88</td>\n","      <td>C places the plate into the middle section of...</td>\n","      <td>545.366795</td>\n","      <td>16360</td>\n","    </tr>\n","    <tr>\n","      <th>2329806</th>\n","      <td>a06ac945-e65f-4259-9563-00bbcf62436f</td>\n","      <td>ffff7cd6-7d12-42d4-ae92-f1bfc2fd6e88</td>\n","      <td>C turns to the dining table.</td>\n","      <td>548.878045</td>\n","      <td>16465</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2329807 rows × 5 columns</p>\n","</div>"],"text/plain":["                                    video_uid  \\\n","0        268b8232-2e94-47c6-928b-99ec175340c5   \n","1        268b8232-2e94-47c6-928b-99ec175340c5   \n","2        268b8232-2e94-47c6-928b-99ec175340c5   \n","3        268b8232-2e94-47c6-928b-99ec175340c5   \n","4        268b8232-2e94-47c6-928b-99ec175340c5   \n","...                                       ...   \n","2329802  a06ac945-e65f-4259-9563-00bbcf62436f   \n","2329803  a06ac945-e65f-4259-9563-00bbcf62436f   \n","2329804  a06ac945-e65f-4259-9563-00bbcf62436f   \n","2329805  a06ac945-e65f-4259-9563-00bbcf62436f   \n","2329806  a06ac945-e65f-4259-9563-00bbcf62436f   \n","\n","                               annotation_uid  \\\n","0        0003222a-d2cb-4e5d-9846-cd0c5ca9ae9c   \n","1        0003222a-d2cb-4e5d-9846-cd0c5ca9ae9c   \n","2        0003222a-d2cb-4e5d-9846-cd0c5ca9ae9c   \n","3        0003222a-d2cb-4e5d-9846-cd0c5ca9ae9c   \n","4        0003222a-d2cb-4e5d-9846-cd0c5ca9ae9c   \n","...                                       ...   \n","2329802  ffff7cd6-7d12-42d4-ae92-f1bfc2fd6e88   \n","2329803  ffff7cd6-7d12-42d4-ae92-f1bfc2fd6e88   \n","2329804  ffff7cd6-7d12-42d4-ae92-f1bfc2fd6e88   \n","2329805  ffff7cd6-7d12-42d4-ae92-f1bfc2fd6e88   \n","2329806  ffff7cd6-7d12-42d4-ae92-f1bfc2fd6e88   \n","\n","                                            narration_text  timestamp_sec  \\\n","0                                         C flips the page    2715.831482   \n","1                          C wipes the book with the cloth    2717.087122   \n","2                             C puts the book in the shelf    2725.421392   \n","3                                         C picks the book    2729.706112   \n","4                                         C flips the page    2732.238932   \n","...                                                    ...            ...   \n","2329802           The man Q walks around the dining table.     533.928115   \n","2329803   The man Q fetches water with a water bottle f...     541.369925   \n","2329804        C opens a refrigerator with his right hand.     542.337345   \n","2329805   C places the plate into the middle section of...     545.366795   \n","2329806                       C turns to the dining table.     548.878045   \n","\n","         timestamp_frame  \n","0                  81474  \n","1                  81512  \n","2                  81762  \n","3                  81891  \n","4                  81967  \n","...                  ...  \n","2329802            16017  \n","2329803            16240  \n","2329804            16269  \n","2329805            16360  \n","2329806            16465  \n","\n","[2329807 rows x 5 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# sort narrations inside the same clip (some narrations are not in the correct order in the original file)\n","df_sorted = df.groupby('annotation_uid', group_keys=False).apply(lambda x: x.sort_values('timestamp_sec'))\n","df_sorted = df_sorted.reset_index(drop=True)\n","df_sorted"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T07:34:21.335664Z","iopub.status.busy":"2024-06-12T07:34:21.335279Z","iopub.status.idle":"2024-06-12T07:34:22.632036Z","shell.execute_reply":"2024-06-12T07:34:22.631150Z","shell.execute_reply.started":"2024-06-12T07:34:21.335638Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 12283/12283 [00:00<00:00, 766760.47it/s]\n"]}],"source":["# retrieve information about clips, useful to compute starting and ending time for the queries ground thruth based on the clip reference system\n","\n","# load ego4d.json\n","ego4d_path = '/kaggle/input/annotations-ego4d/annotations/ego4d.json'\n","with open(ego4d_path, 'r') as f:\n","    ego4d_data = json.load(f)\n","\n","ego4d_data.keys()\n","# create a DataFrame for clips\n","clip_records = []\n","for clip in tqdm(ego4d_data['clips']):    \n","    clip_record = {\n","        'clip_uid': clip['clip_uid'],\n","        'video_uid': clip['video_uid'],\n","        'video_start_sec': clip['video_start_sec'],\n","        'video_end_sec': clip['video_end_sec']\n","    }\n","    clip_records.append(clip_record)\n","clip_df = pd.DataFrame(clip_records)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T07:34:22.633774Z","iopub.status.busy":"2024-06-12T07:34:22.633221Z","iopub.status.idle":"2024-06-12T07:34:22.646327Z","shell.execute_reply":"2024-06-12T07:34:22.645282Z","shell.execute_reply.started":"2024-06-12T07:34:22.633746Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clip_uid</th>\n","      <th>video_uid</th>\n","      <th>video_start_sec</th>\n","      <th>video_end_sec</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000eba33-8d14-446a-b016-19bd50e9a3b9</td>\n","      <td>ab2bf67b-efc0-4448-8c91-a4cecb29691f</td>\n","      <td>0.021029</td>\n","      <td>480.021029</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0014331f-18b1-4200-b4cd-bf55a08aa4fe</td>\n","      <td>a67789f8-3788-4a8d-aba8-9b2c2945d457</td>\n","      <td>1868.087695</td>\n","      <td>2168.087695</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00182baf-e3fe-4bee-9416-825555bc4506</td>\n","      <td>786fdc37-a1a2-4576-83f7-0f8e5da4579a</td>\n","      <td>0.021029</td>\n","      <td>480.021029</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>001fe47b-c00a-4fc7-9f94-40ede6b009f5</td>\n","      <td>b99ceea4-7fa3-407c-9b9f-1347645d23f2</td>\n","      <td>719.954362</td>\n","      <td>1199.954362</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0021cb1c-2009-4469-b4c4-76829b9c1cda</td>\n","      <td>0836e1a4-11e6-4b31-bd39-f8e083fdadb3</td>\n","      <td>1079.987695</td>\n","      <td>1379.987695</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>12278</th>\n","      <td>ffddab3a-dbec-49b0-b114-5a1e3d805efa</td>\n","      <td>8b1d4210-a174-4617-8311-df4b32785d28</td>\n","      <td>612.987695</td>\n","      <td>912.987695</td>\n","    </tr>\n","    <tr>\n","      <th>12279</th>\n","      <td>ffe18e54-5a16-4067-9376-e3d4acb6f8dc</td>\n","      <td>8f0f8606-b696-4bb2-909d-703e4e076d80</td>\n","      <td>270.000000</td>\n","      <td>570.000000</td>\n","    </tr>\n","    <tr>\n","      <th>12280</th>\n","      <td>ffe2261f-b973-4fbd-8824-06f8334afdc5</td>\n","      <td>f7b7c31e-cfdb-4029-b850-9f7ba98e42c2</td>\n","      <td>180.021029</td>\n","      <td>660.021029</td>\n","    </tr>\n","    <tr>\n","      <th>12281</th>\n","      <td>ffe5e9b1-bd91-4744-aa9e-c2f560412ca9</td>\n","      <td>edf2c742-f89c-402b-9228-11fda87201f1</td>\n","      <td>0.000000</td>\n","      <td>480.000000</td>\n","    </tr>\n","    <tr>\n","      <th>12282</th>\n","      <td>ffecaefd-9e0b-48b8-87d7-a329ffdb048c</td>\n","      <td>08314a38-1885-4fa4-bf92-3c6f4185a5cd</td>\n","      <td>2969.987695</td>\n","      <td>3269.987695</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>12283 rows × 4 columns</p>\n","</div>"],"text/plain":["                                   clip_uid  \\\n","0      000eba33-8d14-446a-b016-19bd50e9a3b9   \n","1      0014331f-18b1-4200-b4cd-bf55a08aa4fe   \n","2      00182baf-e3fe-4bee-9416-825555bc4506   \n","3      001fe47b-c00a-4fc7-9f94-40ede6b009f5   \n","4      0021cb1c-2009-4469-b4c4-76829b9c1cda   \n","...                                     ...   \n","12278  ffddab3a-dbec-49b0-b114-5a1e3d805efa   \n","12279  ffe18e54-5a16-4067-9376-e3d4acb6f8dc   \n","12280  ffe2261f-b973-4fbd-8824-06f8334afdc5   \n","12281  ffe5e9b1-bd91-4744-aa9e-c2f560412ca9   \n","12282  ffecaefd-9e0b-48b8-87d7-a329ffdb048c   \n","\n","                                  video_uid  video_start_sec  video_end_sec  \n","0      ab2bf67b-efc0-4448-8c91-a4cecb29691f         0.021029     480.021029  \n","1      a67789f8-3788-4a8d-aba8-9b2c2945d457      1868.087695    2168.087695  \n","2      786fdc37-a1a2-4576-83f7-0f8e5da4579a         0.021029     480.021029  \n","3      b99ceea4-7fa3-407c-9b9f-1347645d23f2       719.954362    1199.954362  \n","4      0836e1a4-11e6-4b31-bd39-f8e083fdadb3      1079.987695    1379.987695  \n","...                                     ...              ...            ...  \n","12278  8b1d4210-a174-4617-8311-df4b32785d28       612.987695     912.987695  \n","12279  8f0f8606-b696-4bb2-909d-703e4e076d80       270.000000     570.000000  \n","12280  f7b7c31e-cfdb-4029-b850-9f7ba98e42c2       180.021029     660.021029  \n","12281  edf2c742-f89c-402b-9228-11fda87201f1         0.000000     480.000000  \n","12282  08314a38-1885-4fa4-bf92-3c6f4185a5cd      2969.987695    3269.987695  \n","\n","[12283 rows x 4 columns]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["clip_df"]},{"cell_type":"markdown","metadata":{},"source":["## Group n consecutive narrations"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T07:34:22.648732Z","iopub.status.busy":"2024-06-12T07:34:22.647703Z","iopub.status.idle":"2024-06-12T07:34:22.656698Z","shell.execute_reply":"2024-06-12T07:34:22.655826Z","shell.execute_reply.started":"2024-06-12T07:34:22.648699Z"},"trusted":true},"outputs":[],"source":["# check if all the narrations in each sample belong to the same clip (annotation_uid and clip_uid don't match exactly)\n","\n","def check_narrations_within_clip(sample_narrations, clip_df):\n","    \n","    if sample_narrations is None:\n","        return False, None, None, None\n","    \n","    video_uid = sample_narrations['video_uid'].iloc[0]\n","    min_timestamp = sample_narrations['timestamp_sec'].min()\n","    max_timestamp = sample_narrations['timestamp_sec'].max()\n","    \n","    # filter clips by video_uid and timestamp range\n","    valid_clips = clip_df[(clip_df['video_uid'] == video_uid) &\n","                          (clip_df['video_start_sec'] <= min_timestamp) &\n","                          (clip_df['video_end_sec'] >= max_timestamp)]\n","    \n","    # if there's exactly one matching clip, all narrations belong to the same clip\n","    if len(valid_clips) == 1:\n","        clip = valid_clips.iloc[0]\n","        return True, clip['clip_uid'], clip['video_start_sec'], clip['video_end_sec']\n","    return False, None, None, None"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T07:34:22.658302Z","iopub.status.busy":"2024-06-12T07:34:22.657987Z","iopub.status.idle":"2024-06-12T07:36:59.771478Z","shell.execute_reply":"2024-06-12T07:36:59.770448Z","shell.execute_reply.started":"2024-06-12T07:34:22.658274Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 44258/44258 [02:34<00:00, 285.89it/s]\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_uid</th>\n","      <th>annotation_uid</th>\n","      <th>narration_text</th>\n","      <th>timestamp_sec</th>\n","      <th>timestamp_frame</th>\n","      <th>clip_uid</th>\n","      <th>video_start_sec</th>\n","      <th>video_end_sec</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>74427b5d-2780-4680-957a-62f532766672</td>\n","      <td>000d16f1-dd58-456c-b6c6-e5e6d91fbb57</td>\n","      <td>C picks a turner spoon from the pan on the fl...</td>\n","      <td>861.178859</td>\n","      <td>25835</td>\n","      <td>f7a11713-a2cf-4f07-a6ee-f94a55f2cc43</td>\n","      <td>694.021029</td>\n","      <td>1173.987695</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>74427b5d-2780-4680-957a-62f532766672</td>\n","      <td>000d16f1-dd58-456c-b6c6-e5e6d91fbb57</td>\n","      <td>C hits the turner spoon on the pan cover with...</td>\n","      <td>863.080549</td>\n","      <td>25892</td>\n","      <td>f7a11713-a2cf-4f07-a6ee-f94a55f2cc43</td>\n","      <td>694.021029</td>\n","      <td>1173.987695</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>74427b5d-2780-4680-957a-62f532766672</td>\n","      <td>000d16f1-dd58-456c-b6c6-e5e6d91fbb57</td>\n","      <td>C lifts the pan cover from the pan on the coo...</td>\n","      <td>865.457859</td>\n","      <td>25963</td>\n","      <td>f7a11713-a2cf-4f07-a6ee-f94a55f2cc43</td>\n","      <td>694.021029</td>\n","      <td>1173.987695</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>74427b5d-2780-4680-957a-62f532766672</td>\n","      <td>000d16f1-dd58-456c-b6c6-e5e6d91fbb57</td>\n","      <td>C places the pan cover on the pan on the floor.</td>\n","      <td>867.361219</td>\n","      <td>26020</td>\n","      <td>f7a11713-a2cf-4f07-a6ee-f94a55f2cc43</td>\n","      <td>694.021029</td>\n","      <td>1173.987695</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>cb6dbc68-561e-4f73-ad98-ba69d4fcf701</td>\n","      <td>0012f8ba-2427-4ef8-9822-d3311a4b5921</td>\n","      <td>C talks to man X</td>\n","      <td>538.899758</td>\n","      <td>16166</td>\n","      <td>02de9500-d574-446e-95fc-3e82c367385a</td>\n","      <td>299.998698</td>\n","      <td>599.965365</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>18551</th>\n","      <td>dc0ef581-bb29-464e-8ada-0953230d8f7d</td>\n","      <td>ffc47179-dcff-441c-a032-ff1edf3ecc34</td>\n","      <td>C walks forward a bit.</td>\n","      <td>586.102258</td>\n","      <td>17582</td>\n","      <td>e355d07d-844b-44ff-bfab-c1fb98d48b6c</td>\n","      <td>539.987695</td>\n","      <td>810.987695</td>\n","    </tr>\n","    <tr>\n","      <th>18552</th>\n","      <td>f262e5b6-6bf0-42b3-b8a6-2ea1b61c75af</td>\n","      <td>ffd7735d-866e-4298-a1fb-111877c9cdfa</td>\n","      <td>C stares in  boutique</td>\n","      <td>2.944149</td>\n","      <td>88</td>\n","      <td>168b41cd-cf36-4b28-b704-bbd500663c9f</td>\n","      <td>0.021029</td>\n","      <td>480.021029</td>\n","    </tr>\n","    <tr>\n","      <th>18553</th>\n","      <td>f262e5b6-6bf0-42b3-b8a6-2ea1b61c75af</td>\n","      <td>ffd7735d-866e-4298-a1fb-111877c9cdfa</td>\n","      <td>C fixes cable on the socket</td>\n","      <td>9.660709</td>\n","      <td>289</td>\n","      <td>168b41cd-cf36-4b28-b704-bbd500663c9f</td>\n","      <td>0.021029</td>\n","      <td>480.021029</td>\n","    </tr>\n","    <tr>\n","      <th>18554</th>\n","      <td>f262e5b6-6bf0-42b3-b8a6-2ea1b61c75af</td>\n","      <td>ffd7735d-866e-4298-a1fb-111877c9cdfa</td>\n","      <td>C operates mobile phone in the room</td>\n","      <td>23.101209</td>\n","      <td>692</td>\n","      <td>168b41cd-cf36-4b28-b704-bbd500663c9f</td>\n","      <td>0.021029</td>\n","      <td>480.021029</td>\n","    </tr>\n","    <tr>\n","      <th>18555</th>\n","      <td>f262e5b6-6bf0-42b3-b8a6-2ea1b61c75af</td>\n","      <td>ffd7735d-866e-4298-a1fb-111877c9cdfa</td>\n","      <td>C stares in the room</td>\n","      <td>166.717829</td>\n","      <td>5001</td>\n","      <td>168b41cd-cf36-4b28-b704-bbd500663c9f</td>\n","      <td>0.021029</td>\n","      <td>480.021029</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>18556 rows × 8 columns</p>\n","</div>"],"text/plain":["                                  video_uid  \\\n","0      74427b5d-2780-4680-957a-62f532766672   \n","1      74427b5d-2780-4680-957a-62f532766672   \n","2      74427b5d-2780-4680-957a-62f532766672   \n","3      74427b5d-2780-4680-957a-62f532766672   \n","4      cb6dbc68-561e-4f73-ad98-ba69d4fcf701   \n","...                                     ...   \n","18551  dc0ef581-bb29-464e-8ada-0953230d8f7d   \n","18552  f262e5b6-6bf0-42b3-b8a6-2ea1b61c75af   \n","18553  f262e5b6-6bf0-42b3-b8a6-2ea1b61c75af   \n","18554  f262e5b6-6bf0-42b3-b8a6-2ea1b61c75af   \n","18555  f262e5b6-6bf0-42b3-b8a6-2ea1b61c75af   \n","\n","                             annotation_uid  \\\n","0      000d16f1-dd58-456c-b6c6-e5e6d91fbb57   \n","1      000d16f1-dd58-456c-b6c6-e5e6d91fbb57   \n","2      000d16f1-dd58-456c-b6c6-e5e6d91fbb57   \n","3      000d16f1-dd58-456c-b6c6-e5e6d91fbb57   \n","4      0012f8ba-2427-4ef8-9822-d3311a4b5921   \n","...                                     ...   \n","18551  ffc47179-dcff-441c-a032-ff1edf3ecc34   \n","18552  ffd7735d-866e-4298-a1fb-111877c9cdfa   \n","18553  ffd7735d-866e-4298-a1fb-111877c9cdfa   \n","18554  ffd7735d-866e-4298-a1fb-111877c9cdfa   \n","18555  ffd7735d-866e-4298-a1fb-111877c9cdfa   \n","\n","                                          narration_text  timestamp_sec  \\\n","0       C picks a turner spoon from the pan on the fl...     861.178859   \n","1       C hits the turner spoon on the pan cover with...     863.080549   \n","2       C lifts the pan cover from the pan on the coo...     865.457859   \n","3        C places the pan cover on the pan on the floor.     867.361219   \n","4                                       C talks to man X     538.899758   \n","...                                                  ...            ...   \n","18551                             C walks forward a bit.     586.102258   \n","18552                              C stares in  boutique       2.944149   \n","18553                        C fixes cable on the socket       9.660709   \n","18554                C operates mobile phone in the room      23.101209   \n","18555                               C stares in the room     166.717829   \n","\n","       timestamp_frame                              clip_uid  video_start_sec  \\\n","0                25835  f7a11713-a2cf-4f07-a6ee-f94a55f2cc43       694.021029   \n","1                25892  f7a11713-a2cf-4f07-a6ee-f94a55f2cc43       694.021029   \n","2                25963  f7a11713-a2cf-4f07-a6ee-f94a55f2cc43       694.021029   \n","3                26020  f7a11713-a2cf-4f07-a6ee-f94a55f2cc43       694.021029   \n","4                16166  02de9500-d574-446e-95fc-3e82c367385a       299.998698   \n","...                ...                                   ...              ...   \n","18551            17582  e355d07d-844b-44ff-bfab-c1fb98d48b6c       539.987695   \n","18552               88  168b41cd-cf36-4b28-b704-bbd500663c9f         0.021029   \n","18553              289  168b41cd-cf36-4b28-b704-bbd500663c9f         0.021029   \n","18554              692  168b41cd-cf36-4b28-b704-bbd500663c9f         0.021029   \n","18555             5001  168b41cd-cf36-4b28-b704-bbd500663c9f         0.021029   \n","\n","       video_end_sec  \n","0        1173.987695  \n","1        1173.987695  \n","2        1173.987695  \n","3        1173.987695  \n","4         599.965365  \n","...              ...  \n","18551     810.987695  \n","18552     480.021029  \n","18553     480.021029  \n","18554     480.021029  \n","18555     480.021029  \n","\n","[18556 rows x 8 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# sample\n","\n","import random\n","n = 4 # we will have n-1 narrations in each sample e.g n=4 -> 3 narrations for each sample, \n","# we take n initially because we use the timestamp of the last+1-th narration as final timestamp for our group sample of narrations\n","\n","# good samples\n","sampling_good_records = []\n","\n","def sample_n_consecutive_records(group, n):\n","    N = len(group)\n","    if N >= n:\n","        i = random.randint(0, N-n)\n","        while i <= N - n: # manage the end of the clip\n","            sample = group.iloc[i:i + n]\n","            # if all the first n-1 narrations extracted are good, then return the sample. \n","            if not sample.iloc[:-1]['narration_text'].str.strip().str.contains(('nsure')).any():\n","                return sample.copy()\n","            # else, we begin our sampling starting from the narration following the last \"#unsure\" one.\n","            unsure_index = sample[sample['narration_text'].str.strip().str.contains(('nsure'))].index[-1] #-1 last one\n","            i = unsure_index + 1 \n","    return None #if no good sample is found, we don't return anything. \n","\n","# group by 'annotation_uid' and do the sampling\n","grouped = df_sorted.groupby('annotation_uid')\n","\n","for annotation_uid, group in tqdm(grouped):\n","    \n","    sample = sample_n_consecutive_records(group, n)\n","    is_valid, clip_uid, video_start_sec, video_end_sec = check_narrations_within_clip(sample, clip_df)\n","    if is_valid:\n","        sample['clip_uid'] = clip_uid\n","        sample['video_start_sec'] = video_start_sec\n","        sample['video_end_sec'] = video_end_sec\n","        \n","        sampling_good_records.append(sample)\n","        \n","samplings_df = pd.concat(sampling_good_records).reset_index(drop=True)\n","\n","samplings_df"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T07:36:59.773400Z","iopub.status.busy":"2024-06-12T07:36:59.772905Z","iopub.status.idle":"2024-06-12T07:37:00.580951Z","shell.execute_reply":"2024-06-12T07:37:00.580043Z","shell.execute_reply.started":"2024-06-12T07:36:59.773365Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 4639/4639 [00:00<00:00, 5824.64it/s]\n"]}],"source":["sample_dicts = [] # list of dictionaries, one per sample\n","\n","for i in tqdm(range(0, len(samplings_df) - n + 1, n)):\n","      \n","    # we can just iterate through them and build the dictionaries because the samples are already sorted\n","    \n","    # retrieve useful fields\n","    text = samplings_df.loc[i:i+n-2, 'narration_text'].tolist()            \n","    start = samplings_df.loc[i, 'timestamp_sec']\n","    end = samplings_df.loc[i+n-1, 'timestamp_sec']\n","    clip_uid = samplings_df.loc[i, 'clip_uid']\n","    video_uid = samplings_df.loc[i, 'video_uid']\n","    video_start_sec = samplings_df.loc[i, 'video_start_sec'] \n","    video_end_sec = samplings_df.loc[i, 'video_end_sec'] \n","    \n","    sample_dict = {\n","        \"text\": text,\n","        \"start\": start,\n","        \"end\": end,\n","        \"clip_uid\": clip_uid,\n","        \"video_uid\": video_uid,\n","        \"video_start_sec\": video_start_sec, # start of the clip inside the video\n","        \"video_end_sec\": video_end_sec # end of the clip inside the video\n","    }\n","   \n","    sample_dicts.append(sample_dict)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T07:37:00.582471Z","iopub.status.busy":"2024-06-12T07:37:00.582161Z","iopub.status.idle":"2024-06-12T07:37:00.588316Z","shell.execute_reply":"2024-06-12T07:37:00.587342Z","shell.execute_reply.started":"2024-06-12T07:37:00.582426Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'text': [' C picks a turner spoon from the pan on the floor with her right hand.', ' C hits the turner spoon on the pan cover with her right hand.', ' C lifts the pan cover from the pan on the cooker with the turner spoon.'], 'start': 861.1788586, 'end': 867.3612186, 'clip_uid': 'f7a11713-a2cf-4f07-a6ee-f94a55f2cc43', 'video_uid': '74427b5d-2780-4680-957a-62f532766672', 'video_start_sec': 694.0210286, 'video_end_sec': 1173.9876952666666}, {'text': [' C talks to man X', ' Man X  taps the black piece', ' Man X points the black piece'], 'start': 538.899757950745, 'end': 542.685037950745, 'clip_uid': '02de9500-d574-446e-95fc-3e82c367385a', 'video_uid': 'cb6dbc68-561e-4f73-ad98-ba69d4fcf701', 'video_start_sec': 299.99869786666665, 'video_end_sec': 599.9653645333333}, {'text': [' C walks on the field ', ' C moves the wood ', ' C holds the woods '], 'start': 581.32364, 'end': 590.58847, 'clip_uid': '740202e6-5d33-45d3-a088-cbd310a14187', 'video_uid': '0b245a61-32d6-4b14-897c-724adad5b231', 'video_start_sec': 292.0, 'video_end_sec': 608.0}, {'text': [' person X holds a cup ', ' person X lifts up a cup from the refrigerator', ' person X holds a coffee mate '], 'start': 980.3339486, 'end': 993.2144686, 'clip_uid': 'fa49f52c-5a5c-4f46-a119-e6837db14d99', 'video_uid': '39af7641-e376-4a99-977f-4d7f746e14ba', 'video_start_sec': 720.0210286, 'video_end_sec': 1200.0210286}, {'text': [' C passes the wooden spatula from his right hand to his left hand.', ' C scrapes some scrambled eggs from the wooden spatula into the frying pan with the spoon in his right hand.', ' C drops the wooden spatula on a transparent pack with his left hand. '], 'start': 240.70875859999998, 'end': 248.8584286, 'clip_uid': 'ebe740e1-e85e-4c28-adff-7df0d55ccd2f', 'video_uid': '7a43089b-fad6-4ba2-8d3f-ba47c2efc61e', 'video_start_sec': 0.0, 'video_end_sec': 308.0}, {'text': [' C presses a button on cooker', ' C stirs meal in pan', ' C passes chopsticks from right to left hand'], 'start': 160.0476386, 'end': 226.6294486, 'clip_uid': 'a7e1d512-0cf9-4135-94cf-3578addc27c8', 'video_uid': 'ba381e74-eafa-443c-ba2f-4ce6450500b0', 'video_start_sec': 0.0210286, 'video_end_sec': 299.98769526666666}, {'text': [' C picks up the impact wrench on the jack lift', ' C tightens the nut on the bolt with the impact wrench', ' C drops the impact wrench on the jack lift'], 'start': 1964.1177366666666, 'end': 1974.7330966666666, 'clip_uid': '668e01f8-9d09-4934-a0fb-ab5e12282e43', 'video_uid': '9c502ef6-41cd-4037-a6be-216587ee8c3e', 'video_start_sec': 1530.0, 'video_end_sec': 2010.0}, {'text': [' C drops the palm frond in his left hand on the ground. ', ' C lifts a palm frond with both hands. ', ' C carries the palm frond with both hands from one part of the farm to another.'], 'start': 4395.668595266667, 'end': 4410.778605266667, 'clip_uid': '43e2b040-5c33-4d51-98cb-192112afece2', 'video_uid': '10217e82-0c41-4ee5-af7d-2e2ecf67b7d1', 'video_start_sec': 3960.0210286, 'video_end_sec': 4440.0210286}, {'text': [' C throws the stick on some pile of stems on the ground', ' C walks forward a bit', ' C cuts a palm stem with the cutlass'], 'start': 4038.9598186, 'end': 4045.6411686, 'clip_uid': '43e2b040-5c33-4d51-98cb-192112afece2', 'video_uid': '10217e82-0c41-4ee5-af7d-2e2ecf67b7d1', 'video_start_sec': 3960.0210286, 'video_end_sec': 4440.0210286}, {'text': [' O moves the dominoes ', ' O sneefs unknown', ' O moves the dominoes '], 'start': 362.6392579507451, 'end': 386.9423979507451, 'clip_uid': '77e09fd6-b560-4439-addc-a1a8a3122046', 'video_uid': '07d824bc-a3fd-4acd-8179-75a7e1e077ce', 'video_start_sec': 299.99869786666665, 'video_end_sec': 599.9653645333333}]\n"]}],"source":["p=10\n","# visualize first p elements of the list\n","print(sample_dicts[:p])"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T07:37:00.590113Z","iopub.status.busy":"2024-06-12T07:37:00.589677Z","iopub.status.idle":"2024-06-12T07:37:00.609661Z","shell.execute_reply":"2024-06-12T07:37:00.608836Z","shell.execute_reply.started":"2024-06-12T07:37:00.590084Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 4639/4639 [00:00<00:00, 509342.07it/s]"]},{"name":"stdout","output_type":"stream","text":["Number of groups of narrations: 4639\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from tqdm import tqdm\n","# narrations: adapt format based on what we need\n","n = 50\n","TAKE_ALL = True # ignore the limit to n\n","list_formatted_narrations = [] # from a list of narrations to a string of concatenated narrations\n","PRINT_NARRATIONS = not TAKE_ALL\n","for i in tqdm(range(len(sample_dicts))):\n","\n","    # join the list elements with '. ' as the separator\n","    formatted_narrations = \". \".join(sample_dicts[i][\"text\"])\n","    if (i<n or TAKE_ALL):        \n","        if PRINT_NARRATIONS:\n","            print(formatted_narrations)\n","            print()\n","            \n","        list_formatted_narrations.append(formatted_narrations)    \n","        \n","print(f\"Number of groups of narrations: {len(list_formatted_narrations)}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Generate new annotations"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T07:37:00.611451Z","iopub.status.busy":"2024-06-12T07:37:00.610942Z","iopub.status.idle":"2024-06-12T09:02:46.259332Z","shell.execute_reply":"2024-06-12T09:02:46.258366Z","shell.execute_reply.started":"2024-06-12T07:37:00.611404Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<start_of_turn>user\n","{prompt}<end_of_turn>\n","<start_of_turn>model\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4639/4639 [1:25:45<00:00,  1.11s/it]\n"]}],"source":["import re\n","from tqdm import tqdm\n","\n","# LLM model\n","\n","list_queries_plus_info = []\n","\n","PRINT_RESULTS = False\n","\n","BREAK_BEFORE_END = False\n","LIMIT_TO_N = 5 # for testing\n","\n","NUM_QUESTIONS = 2 # number of queries requested\n","\n","MODEL_TEMPLATE = \"<start_of_turn>model\\n\"\n","\n","# prompt: start user turn + user prompt + end user turn + start model turn\n","USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n\"+MODEL_TEMPLATE\n","print(USER_CHAT_TEMPLATE)\n","\n","# \"bad prompt\": very detailed, poor quality queries observed with a manual inspection\n","\n","# prompt= \"\"\"You are trying to help humans to augment their memory. \n","#                     To achieve this goal some annotators have watched several videos about people performing different actions in various scenarios, \n","#                     writing down, step by step, short narrations describing in a few words what is happening in the video and the time at which these actions take place. \n","#                     You are requested to generate 2 simple queries that a person could answer just by looking at the video segments corresponding to these narrations:\"\"\"\n","#                     + formatted_narrations[i] +\n","#                     \"\"\"\n","#                     To do so follow these rules:\n","#                     - do not include the answer in the question\n","#                     - avoid introductions, report just the text of the queries one per line\n","\n","# better prompt, but still too detailed: \n","# prompt = \"Generate two short simple queries that a person could answer looking at the video corresponding to these narrations: \"+ list_formatted_narrations[i]+ \"\\nIn your answer report just the queries one per line\"\n","\n","\n","tot = LIMIT_TO_N if BREAK_BEFORE_END else len(list_formatted_narrations)\n","\n","# if LIMIT_TO_N is greater than the length of the list\n","\n","tot = min(tot, len(list_formatted_narrations))\n","\n","# iterate through the formatted grouped narrations and generate queries\n","\n","for i in tqdm(range(tot)):\n","    if i>LIMIT_TO_N and BREAK_BEFORE_END:\n","        break\n","    \n","    # build the prompt with the specific formatted narration\n","    # best prompt, concise and simple\n","    prompt = (\n","        USER_CHAT_TEMPLATE.format(\n","            prompt = \"Generate two questions mixing the information of these sentences: \\\"\"+ list_formatted_narrations[i]+ \"\\\"\\nIn your answer write only two lines with a question each\"\n","        )\n","    )\n","\n","    output = model.generate(\n","        USER_CHAT_TEMPLATE.format(prompt=prompt),\n","        device=device,\n","        output_len=40, # empirically we obtained good results setting length at 20*number_of_queries\n","    )\n","    if PRINT_RESULTS:\n","        print(f\"***\\nNarration: {list_formatted_narrations[i]}\\n***\")\n","        print(\"model:\",output)\n","    \n","    # adjust the output of the LLM\n","    \n","    # split the output into individual queries\n","    output_lines = output.strip().split(\"\\n\")\n","    \n","    # filter queries based on ending with a question mark\n","    def extract_queries(input_list):\n","        # filter the input list to include only lines ending with a question mark\n","        queries = [item for item in input_list if item.strip().endswith('?')]\n","\n","        # remove any leading numbering and formatting\n","        queries = [re.sub(r'(^\\d+\\.\\s*|-+\\s*|>+\\s*)|(^\\w+ \\d+\\:)', '', query) for query in queries]\n","\n","        return queries\n","\n","    # extract the queries\n","    queries = extract_queries(output_lines)\n","    \n","    if PRINT_RESULTS:\n","        # print the result\n","        print(\"extracted:\")\n","        print(queries)\n","    \n","    if queries: # if list not empty\n","        # retrieve information from the sample dictionary\n","        start = sample_dicts[i][\"start\"]\n","        end = sample_dicts[i][\"end\"]\n","        clip_uid = sample_dicts[i][\"clip_uid\"]\n","        video_uid = sample_dicts[i][\"video_uid\"]\n","        video_start_sec = sample_dicts[i][\"video_start_sec\"]\n","        video_end_sec = sample_dicts[i][\"video_end_sec\"]\n","\n","        # build the query entry with necessary information\n","        query_entry = {\n","            'queries': queries,\n","            'start': start,\n","            'end': end,\n","            'clip_uid': clip_uid,\n","            'video_uid': video_uid,\n","            'video_start_sec':video_start_sec,\n","            'video_end_sec':video_end_sec\n","        }\n","        # append the query entry to the list\n","        list_queries_plus_info.append(query_entry)\n","        \n","# flatten the list of queries with their metadata\n","flat_queries = []\n","for entry in list_queries_plus_info:\n","    for query in entry['queries']:\n","        flat_queries.append({\n","            'query': query,\n","            'metadata': entry\n","        })\n","\n","# number of queries to select\n","NUM_QUERIES_TO_SELECT = 8500\n","\n","# randomly select 8500 queries from the list\n","if len(flat_queries) >= NUM_QUERIES_TO_SELECT:\n","    selected_flat_queries = random.sample(flat_queries, NUM_QUERIES_TO_SELECT)\n","else:\n","    raise ValueError(\"The total number of queries is less than 8500\")\n","\n","# create a new structure to hold the selected queries grouped by their original metadata\n","selected_queries_plus_info = []\n","\n","metadata_dict = {}\n","\n","for item in selected_flat_queries:\n","    query = item['query']\n","    metadata = item['metadata']\n","    metadata_key = (\n","        metadata['start'], metadata['end'], metadata['clip_uid'], metadata['video_uid'],\n","        metadata['video_start_sec'], metadata['video_end_sec']\n","    )\n","    if metadata_key not in metadata_dict:\n","        metadata_dict[metadata_key] = {\n","            'queries': [],\n","            'start': metadata['start'],\n","            'end': metadata['end'],\n","            'clip_uid': metadata['clip_uid'],\n","            'video_uid': metadata['video_uid'],\n","            'video_start_sec': metadata['video_start_sec'],\n","            'video_end_sec': metadata['video_end_sec']\n","        }\n","    metadata_dict[metadata_key]['queries'].append(query)\n","\n","selected_queries_plus_info = list(metadata_dict.values())"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T09:02:46.260967Z","iopub.status.busy":"2024-06-12T09:02:46.260675Z","iopub.status.idle":"2024-06-12T09:02:46.266384Z","shell.execute_reply":"2024-06-12T09:02:46.265484Z","shell.execute_reply.started":"2024-06-12T09:02:46.260942Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Narration:  C picks a turner spoon from the pan on the floor with her right hand..  C hits the turner spoon on the pan cover with her right hand..  C lifts the pan cover from the pan on the cooker with the turner spoon.\n","Queries: \n","['What did C pick up with her right hand from the pan on the floor?', 'How did C hit the turner spoon on the pan cover?']\n","Narration:  C talks to man X.  Man X  taps the black piece.  Man X points the black piece\n","Queries: \n","['What did C talk to man X about?', 'What did man X tap on the black piece?']\n","Narration:  C walks on the field .  C moves the wood .  C holds the woods \n","Queries: \n","[' Where does C walk?', ' What does C do with the wood?']\n","Narration:  person X holds a cup .  person X lifts up a cup from the refrigerator.  person X holds a coffee mate \n","Queries: \n","['What is person X holding?', 'In what way is person X raising the cup?']\n","Narration:  C passes the wooden spatula from his right hand to his left hand..  C scrapes some scrambled eggs from the wooden spatula into the frying pan with the spoon in his right hand..  C drops the wooden spatula on a transparent pack with his left hand. \n","Queries: \n","['What did C do with the wooden spatula?', 'How did C use the spoon in his right hand?']\n","Narration:  C presses a button on cooker.  C stirs meal in pan.  C passes chopsticks from right to left hand\n","Queries: \n","['What does C do while pressing a button on the cooker?', 'What does C do when stirring a meal in a pan?']\n","Narration:  C picks up the impact wrench on the jack lift.  C tightens the nut on the bolt with the impact wrench.  C drops the impact wrench on the jack lift\n","Queries: \n","[' What did C pick up on the jack lift?', ' What did C do with the impact wrench?']\n","Narration:  C drops the palm frond in his left hand on the ground. .  C lifts a palm frond with both hands. .  C carries the palm frond with both hands from one part of the farm to another.\n","Queries: \n","['What was C doing with his left hand?', 'How did C hold the palm frond?']\n","Narration:  C throws the stick on some pile of stems on the ground.  C walks forward a bit.  C cuts a palm stem with the cutlass\n","Queries: \n","['What did C throw onto the pile of stems?', 'In what way did C move forward?']\n","Narration:  O moves the dominoes .  O sneefs unknown.  O moves the dominoes \n","Queries: \n","['What does O do to the dominoes that make them move?', 'How does O sneak into the story?']\n"]}],"source":["# visualize some narrations - queries examples\n","for i in range(10):\n","    print(\"Narration: \"+list_formatted_narrations[i])\n","    print(\"Queries: \")\n","    print(list_queries_plus_info[i][\"queries\"])  "]},{"cell_type":"markdown","metadata":{},"source":["## Generate the output JSON"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T09:02:46.268142Z","iopub.status.busy":"2024-06-12T09:02:46.267815Z","iopub.status.idle":"2024-06-12T09:02:47.116482Z","shell.execute_reply":"2024-06-12T09:02:47.115491Z","shell.execute_reply.started":"2024-06-12T09:02:46.268111Z"},"trusted":true},"outputs":[],"source":["# build the automatically-generated-queries annotation file for pretraining\n","import uuid\n","\n","output_data = {        \n","    \"version\":\"2\",\n","    \"date\":\"120624\",\n","    \"description\":\"gemma NLQ Annotations from narrations (train) 3->2\",\n","    \"manifest\":\"\", \n","    \"videos\":[]\n","}\n","    \n","for item in list_queries_plus_info: \n","    \n","    # check if the video entry is already present: if not it creates the whole structure and adds the video to the list\n","    \n","    video_entry = next((video for video in output_data[\"videos\"] if video[\"video_uid\"] == item[\"video_uid\"]), None)\n","    \n","    if not video_entry:\n","        video_entry = {\n","            \"video_uid\": item[\"video_uid\"],\n","            \"clips\": [],\n","            \"split\": \"train\" # used for training\n","        }\n","        output_data[\"videos\"].append(video_entry)\n","        \n","    # same of video for clip level\n","    clip_entry = next((clip for clip in video_entry[\"clips\"] if clip[\"clip_uid\"] == item[\"clip_uid\"]), None)\n","    \n","    \n","    if not clip_entry:\n","\n","        clip_entry = {\n","            \"clip_uid\": item[\"clip_uid\"],\n","            \"video_start_sec\": item[\"video_start_sec\"],\n","            \"video_end_sec\": item[\"video_end_sec\"],\n","            \"video_start_frame\": None,  \n","            \"video_end_frame\": None,     \n","            \"clip_start_sec\": None,\n","            \"clip_end_sec\": None,\n","            \"clip_start_frame\": None,\n","            \"clip_end_frame\": None,\n","            \"source_clip_uid\": \"\",  \n","            \"annotations\": []\n","        }\n","        video_entry[\"clips\"].append(clip_entry)\n","    \n","    # annotation level\n","    \n","    annotation_entry = {\n","        \"language_queries\": [\n","            {\n","                \"clip_start_sec\": float(item['start'] - item[\"video_start_sec\"]), #clip_start_sec,\n","                \"clip_end_sec\": float(item['end'] - item[\"video_end_sec\"]), #clip_end_sec\n","                \"video_start_sec\": item['start'], # not used \n","                \"video_end_sec\": item['end'], # not used\n","                \"video_start_frame\": int(item['start'] * 30), # All the videos have 30 fps, not used\n","                \"video_end_frame\": int(item['end'] * 30), # All the videos have 30 fps, not used\n","                \"template\": \"\",\n","                \"query\": query,\n","                \"slot_x\": \"\",\n","                \"verb_x\": \"\",\n","                \"slot_y\": \"\",\n","                \"verb_y\": \"\",\n","                \"raw_tags\": [\n","                    \"\",\n","                    query,\n","                    \"\",\n","                    \"\",\n","                    \"\",\n","                    \"\"\n","                ]\n","            } for query in item['queries']\n","        ],\n","        \"annotation_uid\": str(uuid.uuid4()),  # Generate a unique ID for the annotation\n","    }\n","    \n","    # if annotations list of the clip is empty\n","    if not clip_entry[\"annotations\"]:\n","        clip_entry[\"annotations\"].append(annotation_entry)\n","    else:\n","        # extend the list with new annotations, [0]-> first (and only) annotator\n","        clip_entry[\"annotations\"][0][\"language_queries\"].extend(annotation_entry[\"language_queries\"])\n","\n","# print(json.dumps(output_data, indent=4))"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T09:02:47.118074Z","iopub.status.busy":"2024-06-12T09:02:47.117781Z","iopub.status.idle":"2024-06-12T09:02:47.827882Z","shell.execute_reply":"2024-06-12T09:02:47.826842Z","shell.execute_reply.started":"2024-06-12T09:02:47.118048Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Output written to /kaggle/working/nlq_train_gen.json\n"]}],"source":["# define the output file path\n","output_file_path = \"/kaggle/working/nlq_train_gen.json\"\n","\n","# write the data to the JSON file\n","with open(output_file_path, \"w\") as json_file:\n","    json.dump(output_data, json_file, indent=4)\n","\n","# print the output file path to confirm the file has been written\n","print(f\"Output written to {output_file_path}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5092751,"sourceId":8527956,"sourceType":"datasetVersion"},{"datasetId":5166785,"sourceId":8629487,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelInstanceId":5383,"sourceId":11358,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
